<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Specification Goes First</title>
  <link href="../Styles/Global.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <h1 id="sigil_toc_id_9">Specification Goes First</h1>

  <h2>What's The Point of Writing Specification After The Fact?</h2>

  <p>In the last chapter, I said that a test is in reality a statement of a specification. If so, then it's really a matter of consistency to specify what we're going to write before we attempt to write it. Does the other way round even make sense? A specification written after the implementation is complete is just a matter of documenting existing solution. Sure, it can be valuable when the effort is done as a kind of reverse-engineering (i.e. writing specification for something that was implemented long ago and we don't really know the exact business rules or policies, which we discover as we document the existing solution), but doing it just after we made all the decisions seems like a waste of time, not to mention that it's dead boring (Don't believe me? Try to implement a simple calculator app and the write specification for it as soon as it's implemented and working).</p>

  <p>Oh, and did I tell you that without a specification of some kind we don't really know whether we're done or not (because in order to know it, we need to compare the implemented functionality to 'something', even if this 'something' is only in the customer's head).</p>

  <p>In the previous chapter, I told you that one of the differences between usual textual specification and our Specification consisting of executable Statements is that we don't write our Specification fully up-front. The usual sequence is to specify a bit and code a bit, then repeat. When doing TDD, we're going repeatedly through few phases in cycles. We like these cycles to be short, so that we can get a quick feedback. Being able to get this quick feedback is essential, because it allows us to move forward with confidence that what we already have works as we intended. Also, it allows us to use the knowledge we gained in the previous cycle to make the next cycle more efficient.</p>

  <p>Talking about cycles, it's no surprise that the traditional illustration of the TDD process is modeled visually as a circle-like flow:</p>

  <p><img alt="Traditional_TDD_cycle" src="../Images/Traditional_TDD_cycle.png" /></p>

  <p>Note that this form uses the traditional terminology, so before I explain the steps, I'll translate it to use our terms of Specification and Statements:</p>

  <p><img alt="Modern_TDD_cycle" src="../Images/Modern_TDD_cycle.png" /></p>

  <p>Now it seems more in place - specifying how something should behave before putting that behavior in place is way more intuitive than testing something that does not exist.</p>

  <p>Anyway, these three steps demand some explanations. In the coming chapters, I'll give you some examples of how this process works in practice and introduce an expanded version, but in the meantime, its sufficient to say that:</p>

  <dl>
    <dt>Write an unfulfilled Statement</dt>

    <dd>means that the statement shows on the test list as unfulfilled (in most xUnit frameworks, it will be marked with red color)</dd>

    <dt>Fulfill it</dt>

    <dd>means that we write just enough code to fulfill the Statement (in most xUnit frameworks, the fulfilled Statement will be marked with green color)</dd>

    <dt>Refactor</dt>

    <dd>is a step that I have silently discarded so far (and will do so for at least few next chapters. Don't worry, we'll get back to it). Basically, it boils down to using the coverage of executable specification we already have in place to safely enhance the quality of the covered code.</dd>
  </dl>

  <p>By the way, this process is sometimes refered to as "Red-Green-Refactor". I'm just mentioning it here for the record - I don't plan on using this term further in the book.</p>

  <p>Anyway, try to look at the drawing with TDD process again - can you spot a single word I underlined? Yes, it's <strong>unfulfilled</strong>. It means that when you write a Statement, you have to run it and watch it fail its assertions before fulfilling it. Why is that so important? Isn't it just enough to write the specification first? Why run it and watch it fail?</p>

  <p>I'd like to use it to elaborate on the few reasons I consider Test First to be an essential practice:</p>

  <h3>You don't know whether the Statement can ever be false</h3>

  <p>Accurate Statement fails when it isn't fulfilled and passes when it is. Also, it stays in place and starts failing as soon as the code stops fulfilling this Statement (e.g. as a result of mistake made during refactoring). When your run a Statement after it's implemented, you don't even know whether it really describes a need accurately? You didn't ever watch it fail, so how do you know it ever will?</p>

  <p>The first time I encountered this argument (it was before I started thinking of unit tests as executable specification), it quickly raised my self-defense mechanism: "But how can it be?" - I thought - "I'm a wise person, I know what code I'm writing. If I make my unit tests small enough, it's self-evident that I'm describing the correct behavior. This is paranoid". However, this turned out not to be true. Let me describe, from my experience, just three ways (there are more, I just forgot the rest :-D) one can really put in a Statement that is false-positive (i.e. it cheats you into thinking it's fulfilled even when it's not):</p>

  <h4>Accidentally skipping of adding Statement to Specification.</h4>

  <p>However funny this may sound, it happens. The example I'm going to give is from C#, but almost every xUnit framework in almost every language has some kind of mechanism of marking methods as Statements, whether by attributes (C#, e.g. xUnit.Net's <code>Fact</code> attribute) or annotations (Java) or with macros (C and C++) or by inheriting from common class, or just a naming convention.</p>

  <p>Let's take xUnit.Net as an example. As I stated previously, In xUnit.Net, to make a method a Statement, you mark with <code>[Fact]</code> attribute in the following way:</p>
  <pre class="brush:csharp;">public class CalculatorSpecification
{
  [Fact]
  public void ShouldDisplayAdditionResultAsSumOfArguments() 
  {
    //... 
  }
}
</pre>

  <p>Now, imagine that you're writing this Statement post-factum as a unit test in an environment that has, let's say, more than thirty Statements - you've written the code, now you're just creating a test after test "to ensure" (as you see, this is not my favourite reason for writing unit tests) the code works. Code, test - pass, test - pass, test - pass. You almost always launch the whole Specification, since it's usually painful to point out each time, which Statement you wanna evaluate, plus, you don't want to introduce regression. So, this is really: Code, Test - all pass, test - all pass, test - all pass... Hopefully, you use some kind of snippets mechanism for creating new Statements, but if not (and many don't actually do this), once in a while, you do something like this:</p>
  <pre class="brush:csharp;">
public class CalculatorSpecification
{
  //... some Statements here

  public void ShouldDisplayZeroWhenResetIsPerformed()
  {
    //... 
  }
}
</pre>

  <p>And you don't even notice that this will not be run with the rest of the Specification, because there are so many Statements already that it's almost irrational to search for your each added Statement in the list and make sure it's there. Also, this fact that you have omitted the addition, does not disturb your work flow: test - all pass, test - all pass, test - all pass... So, what you end up is a Statement that not only will never be false - <strong>it will never be executed</strong>.</p>

  <p>So, why does treating tests as Statements and writing them first help here?&nbsp; <strong>Because then, a Statement that starts off as being evaluated as true is what DOES disturb your work flow.</strong> In TDD, the work flow is: Statement - unfulfilled - fulfilled (ok, and refactor, but for the sake of THIS discussion, it doesn't matter so much), Statement - unfulfilled - fulfilled, Statement - unfulfilled - fulfilled...&nbsp;</p>

  <h4>Misplacing mock setup</h4>

  <p>Ok, this may sound even funnier (well, honestly, most mistakes sound funny), but it happened to me a couple of times, so makes sense to mention. The example I'm going to show uses manual mocks, but this can happen with dynamic mocks as well, especially if you're in a hurry.</p>

  <p>Let's take a look at the following Statement saying that setting a value higher than allowed to a field of a frame should produce error result:</p>
  <pre class="brush:csharp;">[Fact]
public void ShouldRecognizeTimeSlotAboveMaximumAllowedAsInvalid()
{
  //GIVEN
  var frame = new FrameMock(); //manual
  var validation = new Validation();
  var timeSlotAboveMaximumAllowed = TimeSlot.MaxAllowed + 1;

  //WHEN
  var result = validation.PerformForTimeSlotIn(frame);
  frame.GetTimeSlot_Returns 
    = timeSlotAboveMaximumAllowed;

  //THEN
  Assert.False(result.Passed);
  Assert.Equal(
    ValidationFailureReasons.AboveAcceptableLimit, 
    result.Reason);
}
</pre>

  <p>Note how the method <code>PerformForTimeSlotIn()</code>, which triggers the specified behavior is accidentally called BEFORE the mock is actually set up and the set up return value is never taken into account, which is not uncommon in the heat of the battle. By the way, how did it happen that, despite this fact of messing up the mock setup, the end result was still correct? Well, it sometimes turns out like this, most often in case of various boundary values (nulls etc.).</p>

  <h4>Using static data inside production code</h4>

  <p>Once in a while, you have to jump in and add some new Statements to some class Specification and some logic to the class itself. Let's assume that the class and its existing specification was written by someone else. Imagine this code is a wrapper around your product XML configuration file. You decide to write your Statements AFTER applying the changes ("well", you can say, "I'm all protected by the suite that's already in place, so I can make my change without risking regression, then just test my changes and it's all good...").</p>

  <p>So, you start writing the new Statement. The Specification class already contains a field member like this:</p>
  <pre class="brush:csharp;">XmlConfiguration config = new XmlConfiguration(xmlFixtureString);
</pre>

  <p>What it does is to set up a single object used by all the Statements. So, each Statement uses the same <code>config</code> object initialized with the same <code>xmlConfiguration</code> string. The string is already pretty huge and messy, since it was made to contain what's required by all existing Statements. What you need to write tests for is a little corner case that does not need all this crap that's in this string. So, you decide to start fresh and create a separate object of <code>XmlConfiguration</code> class with your own, minimal string. Your Statement begins like this:</p>
  <pre class="brush:csharp;">string customFixture = CreateMyOwnFixtureForThisTestOnly();
var configuration = new XmlConfiguration(customFixture);
...
</pre>

  <p>And it passes - cool... not. Ok, what's wrong with this? Nothing big, unless you read the source code of XmlConfiguration class carefully. Inside, you can see, how the xml string is stored:</p>
  <pre class="brush:csharp;">private static string xmlText; //note the static keyword!
</pre>

  <p>What the...? Well, well, here's what happened: the author of this class coded in a small little optimization. He thought: "The configuration is only changed by members of the support staff and to do it, they have to shut down the system, so, there is no need to read the XML file every time an XmlConfiguration object is created. I can save some CPU cycles and I/O operations by reading it only once when the first object is created. Another created object will just use the same XML!". Good for him, not so good for you. Why? Because (unless your test runs first), your custom xml string will never actually be used!</p>

  <h3>"Test-After" ends up as "Test-Never"</h3>

  <p>Ever had to write a requirement or design document for something that you already implemented? Was it fun? Was it valuable? Was it creative? No, I don't think so. The same is with our executable specification. After we write the code, we have little motivation to specify them - some of the pieces of code "we can just see are correct", other pieces "we already saw working" when we copied our code over to our deployment machine... The design is ready... Specification? Maybe next time...</p>

  <p>Another reason might be time pressure. Let's be honest - we're all in a hurry, we're all under pressure and when this pressure is too high, it triggers heroic behaviors in us, especially when there's a risk of not making it with the sprint commitment. Such heroic behavior usually goes by the following rules: drop all the "baggage", stop learning and experimenting, revert to all of the old "safe" behaviors and "save what we can!". If Specification is written at the end, it is often sacrificed, since the code is already written, "and it will be tested anyway by real tests" (box tests, smoke tests, sanity tests etc.). It is quite the contrary when starting with a Statement, where the Statement evaluating to false is a reason to write any code. Thus, if we want to write code, Specification become irremovable part of your development. By the way, I bet in big corporations no one sane ever thinks they can abandon checking in the code to source control, at the same time treating Specification as "an optional addition".</p><!-- TODO -->

  <p><b style="font-family: Arial, Helvetica, sans-serif;">3) Not doing Test First is a waste of time.</b><br /></p>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    One day, I was listening to <a href="http://www.confreaks.com/videos/759-rubymidwest2011-keynote-architecture-the-lost-years" target="_blank">Robert C. Martin's keynote at Ruby Midwest 2011, called Architecture The Lost Years</a>. At the end, Robert made some digressions, one of them being about TDD. He said that writing unit tests after the code is not TDD. It is a waste of time.&nbsp;
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    The first time I thought about it, I thought it was only about missing all the benefits that Test First brings you: the ability for a test to fail, ability to do a clean-sheet analysis, ability to do Need Driven Design etc., however, now I think there is more to it. If you're reading the <a href="http://sustainabletdd.com/" target="_blank">Sustainable Test Driven Development</a> blog regularly, you know that Amir and Scott value <i>testability</i> as design quality, along with cohesion, encapsulation and others. Also, they state that in order to make TDD (and even plain unit testing for that matter) sustainable, the code must have this testability quality on a very high level. How can we use this valuable insight to identify the waste? Let's see how testability looks like in Test First workflow (let's assume that we're creating new code, not adding stuff to dirty, ugly legacy code):
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <div style="font-family: Verdana,sans-serif;">
    - Write unit test that fails <b><span style="color: #274e13;">(</span></b><i><b><span style="color: #274e13;">The code has high testability</span></b></i><b><span style="color: #274e13;">)&nbsp;</span></b>
  </div>

  <div style="font-family: Verdana,sans-serif;">
    - Write code that satisfies the test
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    Now, how does it usually look like in Test After approach (from what I saw in various situations):
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <div style="font-family: Verdana,sans-serif;">
    - Write some code (probably spans few classes until we're satisfied).
  </div>

  <div style="font-family: Verdana,sans-serif;">
    <i><b>- Start writing unit tests</b></i>
  </div>

  <div style="font-family: Verdana,sans-serif;">
    <i><b>- Notice that unit testing the whole set of classes is cumbersome and unsustainable and contains high redundancy.</b></i>
  </div>

  <div style="font-family: Verdana,sans-serif;">
    <i><b>- Refactor the code to be able to isolate objects and inject some mocks <span style="color: #274e13;">(The code has high testability)</span></b></i>
  </div>

  <div style="font-family: Verdana,sans-serif;">
    - Write proper unit tests.
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    As you may have noticed, I emphasized few steps that are additional in Test After approach. What's their equivalent in Test First? <b>Nothing!</b> Doing these things is a waste of time! And, this is a waste of time I'm seeing done over and over again!
  </div>

  <div style="font-family: Arial,Helvetica,sans-serif;">
    <br />
  </div>

  <p><span style="font-family: Arial,Helvetica,sans-serif;">Anyway, because of this, let me say it clearly: if you've got a guy in your team that's doing TDD and he's the only one, and you're just "coding and shipping" carelessly even without unit tests, be informed, that this guy will ALWAYS have to corect your code to make it testable and, for that matter, better designed. So, if you really wanna stay on the dark side and not do what's right and ethical (which is to do TDD), at least buy him a soft drink from time to time.</span><br />
  <br /></p>
</body>
</html>
